#!/usr/bin/env python

import datetime
import os
import re
import subprocess
import sys

sys.path.insert(1, os.path.join(sys.path[0], "../.."))

import click
import polars

# import dot_init  # noqa: E402, F401

TABLES_DATA_ANONYMIZE = [
    "userprofile_user",
]

TABLES_DATA_SKIP = [
    "async_job_taskmonitor",
    "auditlog_logentry",
    "django_celery_results_taskresult",
    "django_session",
    "integrations_*",
    "order_orderhistoryentry",
    "order_orderhistoryentry",
    "order_ordereditemhistory",
]


@click.command()
@click.option(
    "--uri",
    required=True,
    help="source database uri, e.g. 'postgresql://user:pwd@host:port/db'",
)
@click.option(
    "--backup-dir",
    required=True,
    help="backup dir for data files",
)
@click.option(
    "--bucket",
    required=False,
    default="lt-db-dev",
    help="aws upload bucket, e.g. 'snapshots'",
)
def db_snapshot(uri: str, backup_dir: str, bucket: str):
    print(f"db snapshot starting at {_time_now()}")

    if not os.path.exists(backup_dir):
        os.mkdir(backup_dir)

    table_names = _db_tables_list(uri=uri)

    for table_name in table_names:
        _db_table_export_schema(
            uri=uri,
            table_name=table_name,
            schema_file=f"{backup_dir}/{table_name}.schema.sql",
        )

        if any(
            table_name == table_regex or re.search(table_regex, table_name)
            for table_regex in TABLES_DATA_SKIP
        ):
            _print_warning(f"table {table_name} skip data")
            continue

        if table_name in TABLES_DATA_ANONYMIZE:
            _db_table_anonymize_csv(
                uri=uri,
                table_name=table_name,
                csv_file=f"{backup_dir}/{table_name}.data.csv",
            )
        else:
            _db_table_export_csv(
                uri=uri,
                table_name=table_name,
                csv_file=f"{backup_dir}/{table_name}.data.csv",
            )

    _print_ok(f"db snapshot completed at {_time_now()}")

    # _db_upload(bucket=bucket, path=dump_path)


def _db_table_anonymize_csv(uri: str, table_name: str, csv_file: str) -> list[str]:
    print(f"table anonymize {table_name} csv", end=" ... ", flush=True)

    query = f"select * from {table_name}"
    df_in = polars.read_sql(query, uri)

    if table_name == "userprofile_user":
        df_out = df_in.with_columns(
            [
                polars.when(polars.col("email").str.contains("leaftrade.com"))
                .then(polars.col("email"))
                .otherwise(polars.col("email").hash().cast(polars.Utf8) + "@gmail.com")
                .alias("email"),  # overwrite email value
                polars.col("quicksight_name").fill_null(""),
                polars.col("hubspot_profile").fill_null(""),
            ]
        )
    else:
        raise "whoops"

    df_out.write_csv(csv_file)

    _print_ok("ok")


def _db_table_export_csv(uri: str, table_name: str, csv_file: str) -> list[str]:
    print(f"table export {table_name} csv", end=" ... ", flush=True)

    subprocess.run(
        [
            "psql",
            "-d",
            uri,
            "-c",
            f"\copy (select * from {table_name}) to '{csv_file}' delimiter ',' csv header",
        ],
        check=True,
        capture_output=True,
        stdout=None,
        stderr=None,
    )

    _print_ok("ok")


def _db_table_export_schema(uri: str, table_name: str, schema_file: str) -> list[str]:
    print(f"table export {table_name} schema", end=" ... ", flush=True)

    subprocess.run(
        [
            "pg_dump",
            "-d",
            uri,
            "-s",
            "-t",
            table_name,
            "--no-owner",
            "--no-acl",
            "-f",
            schema_file,
        ],
        check=True,
        capture_output=True,
        stdout=None,
        stderr=None,
    )

    _print_ok("ok")


def _db_tables_list(uri: str) -> list[str]:
    result = subprocess.run(
        [
            "psql",
            "-d",
            uri,
            "-c",
            "\dt public.*",
        ],
        check=True,
        capture_output=True,
        stdout=None,
        stderr=None,
    )

    table_names = []
    result_lines = result.stdout.decode("utf-8").split("\n")

    for line in result_lines[3:]:
        if not line or "|" not in line:
            continue
        schema_name, table_name, _, _ = line.split("|")
        schema_name = schema_name.strip()
        table_name = table_name.strip()

        if schema_name != "public":
            continue

        # if not table_name.startswith("user"):
        #     continue

        table_names.append(table_name)

    return table_names


# def _db_dump(uri: str) -> str:
#     try:
#         db_dump_file = f"./db.{ulid.new().str}.dump"

#         cmd = f"pg_dump -Fc -d {uri} --exclude-table-data=public.auditlog_logentry -f {db_dump_file} --no-acl"

#         subprocess.run(cmd.split(" "), check=True)

#         return db_dump_file
#     except Exception as e:
#         raise e


def _db_upload(bucket: str, path: str):
    aws_profile = "StagingOps"
    s3_dir_name = ""

    _s3_ls_cmd = (
        f"aws --profile {aws_profile} s3 ls --human-readable {s3_dir_name}".split(" ")
    )

    # objects = s3_bucket.list_objects()

    # print(objects)

    # s3_bucket.upload_from_path(path, path)


def _print_error(s: str):
    print("\x1b[1;31m" + s + "\x1b[0m")


def _print_ok(s: str):
    print("\x1b[1;32m" + s + "\x1b[0m")


def _print_warning(s: str):
    print("\x1b[1;33m" + s + "\x1b[0m")


def _time_now() -> str:
    return str(datetime.datetime.now()).split()[1]


if __name__ == "__main__":
    db_snapshot()
