#!/usr/bin/env python

import os
import sys

sys.path.insert(1, os.path.join(sys.path[0], "../.."))

import langchain.callbacks.manager
import langchain.callbacks.streaming_stdout
import langchain.chains
import langchain.embeddings.sentence_transformer
import langchain.text_splitter
import langchain_community.document_loaders
import langchain_community.llms
import langchain_community.vectorstores
from langchain import hub

import dot_init

# Load the document using a LangChain text loader
loader = langchain_community.document_loaders.TextLoader("./data/sotu_2023.txt")
documents = loader.load()

# Split the document into chunks
text_splitter = langchain.text_splitter.CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# Use the sentence transformer package with the all-MiniLM-L6-v2 embedding model
embedding_model = langchain.embeddings.sentence_transformer.SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
db_name = "faiss/sotu"

# Load the text embeddings in SQLiteVSS in a table named sotu
# texts = [doc.page_content for doc in docs]
# db = langchain_community.vectorstores.SQLiteVSS.from_texts(
#     texts = texts,
#     embedding = embedding_model,
#     table = "sotu",
#     db_file = "./sqlite/vss.db"
# )
# print(f"sqlite docs {db.ntotal}")

# persist data
db = langchain_community.vectorstores.FAISS.from_documents(docs, embedding=embedding_model)
db.save_local(db_name)

# load data
db = langchain_community.vectorstores.FAISS.load_local(db_name, embedding_model, allow_dangerous_deserialization=True)

print(f"faiss docs {db.index.ntotal}")

# First, we will do a simple retrieval using similarity search query
question = "What did the president say about Nancy Pelosi?"
data = db.similarity_search(question)

# print results
# print(data[0].page_content)

llm = langchain_community.llms.Ollama(
    model="llama2:7b",
    verbose=True,
    callback_manager=langchain.callbacks.manager.CallbackManager(
        [langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler()]
    ),
)

# LangChain Hub is a repository of LangChain prompts shared by the community
QA_CHAIN_PROMPT = hub.pull("rlm/rag-prompt-llama")
# QA_CHAIN_PROMPT = hub.pull("rlm/rag-prompt")

print(QA_CHAIN_PROMPT)

qa_chain = langchain.chains.RetrievalQA.from_chain_type(
    llm,
    retriever=db.as_retriever(), # we create a retriever to interact with the db using an augmented context
    chain_type_kwargs={
        "prompt": QA_CHAIN_PROMPT
    },
)

result = qa_chain({"query": question})