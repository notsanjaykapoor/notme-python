#!/usr/bin/env python

import os
import sys
import time

sys.path.insert(1, os.path.join(sys.path[0], "../.."))

import click
import llama_cpp
import polars

import dot_init

import models
import services.corpus
import services.corpus.fs
import services.corpus.llm
import services.corpus.models
import services.corpus.vector
import services.database
import services.images
import services.qdrant

@click.group()
def cli():
    pass


@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.pass_context
def delete(ctx, name: str) -> list[str]:
    """
    
    """
    _print_status(f"corpus delete '{name}'")

    with services.database.session.get() as db_session:
        delete_code = services.corpus.delete_by_name(db_session=db_session, name=name)

    _print_status(f"corpus delete '{name}' - result {delete_code}")


@click.command()
@click.option('--uri', default=None, required=True, help="image uri to caption")
@click.pass_context
def image_caption(ctx, uri: str) -> dict:
    """
    caption specified image
    """

    caption_result = services.images.caption(uri=uri)

    print(caption_result)


@click.command()
@click.option('--source-uri', default=None, required=True, help="image caption directory")
@click.pass_context
def image_caption_dir(ctx, source_uri: str) -> dict:
    """
    caption images in source directory
    """
    local_files = services.corpus.fs.files_path_list(source_uri=source_uri, filter="")
    local_files = [file for file in local_files if file.endswith("index.csv")]

    if not local_files:
        raise ValueError("source directory does not have an index file")

    _print_status(f"caption images '{local_files}'")

    for file_path in local_files:
        index_df = polars.read_csv(file_path)
        changes = 0

        for index, row in enumerate(index_df.iter_rows(named=True)):
            if row.get("caption"):
                continue

            image_uri = row.get("uri")

            _print_status(f"caption image '{image_uri}")

            caption_result = services.images.caption(uri=image_uri)
            index_df[index, "caption"] = caption_result.text

            changes += 1

        if changes:
            index_df.write_csv(file_path)

        _print_ok(f"caption images '{file_path} changes {changes} ok")


@click.command()
@click.option('--source-uri', default=None, required=True, help="corpus directory, e.g. file://localhost/./data/rag/yyy")
@click.option('--model', default="auto", required=False, help="embedding model name, e.g. local:gte-large, clip:default, gpt4all, openai")
@click.pass_context
def ingest(ctx, source_uri: str, model: str) -> dict:
    """
    
    """
    with services.database.session.get() as db_session:
        corpus = services.corpus.get_by_source_uri(db_session=db_session, source_uri=source_uri)

        if not corpus:
            corpus = services.corpus.create(
                db_session=db_session,
                epoch=0,
                model=model,
                org_id=0,
                params={
                    "meta": {},
                    "model_dims": 0,
                },
                source_uri=source_uri,
                state=models.corpus.STATE_DRAFT,
            )

        # update corpus epoch
        corpus.epoch = corpus.epoch + 1
        db_session.add(corpus)
        db_session.commit()

        t_start = time.time()

        _print_status(f"corpus {corpus.id} name '{corpus.name}' model '{corpus.model_name}' epoch {corpus.epoch} ingest starting")

        _ingest_result = services.corpus.ingest(
            db_session=db_session,
            corpus_id=corpus.id,
        )

        t_end = time.time()

        _print_status(f"corpus {corpus.id} name '{corpus.name}' model '{corpus.model_name}' epoch {corpus.epoch} ingest completed in {t_end-t_start} seconds")


@click.command()
@click.option('--source-uri', default=None, required=True, help="corpus directory, e.g. file://localhost/./data/rag/yyy")
@click.option('--device', default="auto", required=False, help="torch device to use")
@click.option('--model', default="local:gte-large", required=False, help="embedding model name, e.g. gpt4all, openai, gte-base, gte-large, nomic-embed-text-v1")
@click.option('--splitter', default="chunk:1024:40", required=False, help="text splitter, e.g. semantic, chunk:1024:40")
@click.pass_context
def ingest_bm(ctx, source_uri: str, device: str, model: str, splitter: str) -> dict:
    """
    benchmark ingest on local device
    """
    import llama_index.core.vector_stores.simple
    import llama_index.readers
    import llama_index.vector_stores

    local_files = services.corpus.fs.files_path_list(source_uri=source_uri, filter="")

    if device == "auto":
        device = services.corpus.torch_device()

    _print_status(f"ingest '{source_uri}' device '{device}' model '{model}' files '{local_files}'")

    file_docs = services.corpus.load_docs(files=local_files)

    _, chunk_size, chunk_overlap = splitter.split(":")
    text_splitter = llama_index.core.node_parser.SentenceSplitter(
        chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap),
    )

    doc_nodes = text_splitter.get_nodes_from_documents(file_docs)

    vector_store = llama_index.core.vector_stores.simple.SimpleVectorStore()

    vector_storage_context = llama_index.core.StorageContext.from_defaults(
        vector_store=vector_store,
    )

    t_start = time.time()

    model_klass = services.corpus.models.resolve(model=model, device=device)

    _vector_index = llama_index.core.VectorStoreIndex(
        doc_nodes,
        embed_model=model_klass,
        show_progress=True,
        storage_context=vector_storage_context,
        store_nodes_override=True,
    )

    time_seconds = (time.time() - t_start)

    _print_status(f"ingest '{source_uri}' device '{device}' completed in {time_seconds} seconds")


@click.command()
@click.option('--limit', default=50, required=False, help="search limit")
@click.option('--query', default="", required=False, help="search query")
@click.pass_context
def list(ctx, limit: int, query: str="") -> list[str]:
    """
    List postgres corpus stores
    """
    with services.database.session.get() as db_session:
        list_result = services.corpus.list(db_session=db_session, query=query, offset=0, limit=limit)
    
    objects = list_result.objects

    _print_status(f"corpus list - {len(objects)} results")

    for i, object in enumerate(objects):
        _print_status(f"{i+1}: {object.name}")


@click.command()
@click.option('--limit', default=50, required=False, help="search limit")
@click.option('--query', default="", required=False, help="search query")
@click.pass_context
def list_vector(ctx, limit: int, query: str="") -> list:
    """
    List vector stores
    """
    objects = services.faiss.list()

    _print_status(f"vector list - {len(objects)} results")

    for i, object in enumerate(objects):
        if not(query) or (query in object.get("collection_name")):
            _print_status(f"{i+1}: {object}")


@click.command()
@click.option('--name', default=None, required=True, help="corpus encoded name")
@click.option('--query', default=None, required=True, help="question")
@click.option('--mode', default=None, required=True, help="query mode - query, rag")
@click.pass_context
def query(ctx, name: str, query: str, mode: str, limit: int=10) -> dict:
    """
    
    """
    _print_status(f"corpus query '{name}' mode '{mode}' query '{query}'")

    if mode not in ["query", "rag"]:
        raise ValueError("mode invalid")

    with services.database.session.get() as db_session:
        corpus = services.corpus.get_by_name(db_session=db_session, name=name)

        if not corpus:
            raise ValueError("corpus invalid")

        if mode == "rag":
            search_result = services.corpus.vector.search(corpus=corpus, query=query, limit=2)

            for node in search_result.nodes:
                print(node) # xxx

            llm = llama_cpp.Llama(
                model_path=os.environ.get("APP_LLM_TEXT_PATH"),
                n_ctx=2048,
                verbose=False,
            )

            scope = "\n".join(node.text for node in search_result.nodes)

            # not sure why this doesn't work
            # services.corpus.llm.query(llm=llm, context=context, question=query)

            t_start = time.time()

            # response = llm(
            #     "Q: Name the planets in the solar system? A: ",
            #     max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window
            #     stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
            #     echo=True, # Echo the prompt back in the output
            # )
            response = llm(
                services.corpus.llm.prompt_text(scope=scope, query=query),
                max_tokens=None, # set to None to generate up to the end of the context window
                stop=["Q:", "\n"], # stop generating just before the model would generate a new question
                temperature=0.1,
            )

            t_end = time.time()

            _print_status(f"corpus query '{name}' inference in {t_end-t_start} seconds:")
            _print_status(f"{response}")

        elif mode == "query":
            nodes_result = services.corpus.vector.search(corpus=corpus, query=query, limit=limit)

            print(f"nodes result, len {len(nodes_result.nodes)}:")
            _print_nodes(nodes=nodes_result.nodes)


@click.command()
@click.pass_context
def scan(ctx) -> dict:
    """
    """
    _print_status("corpus scan")

    with services.database.session.get() as db_session:
        scan_result = services.corpus.scan(db_session=db_session, mode="update")

        _print_status(f"corpus scanned {scan_result.scan_count} dirty {len(scan_result.dirty_objects)}")

        for corpus in scan_result.dirty_objects:
            _print_status(f"corpus {corpus.id} name '{corpus.name}' dirty")


@click.command()
@click.pass_context
def sync(ctx) -> dict:
    """
    """
    with services.database.session.get() as db_session:
        corpus_list = services.corpus.list(db_session=db_session, query="", offset=0, limit=1024)
        corpus_objects = corpus_list.objects

        client = services.qdrant.client()

        qdrant_collections = client.get_collections().collections
        qdrant_collection_names = [collection.name for collection in qdrant_collections]

        for corpus in corpus_objects:
            # get corpus vector store
            collection_names = []

            if corpus.vector_img_uri:
                collection_names.append(corpus.vector_img_uri.split(":")[-1])

            if corpus.vector_txt_uri:
                collection_names.append(corpus.vector_txt_uri.split(":")[-1])

            print(f"corpus sync '{corpus.name}' with collections '{collection_names}'", end=" ... ", flush=True)

            for name in collection_names:
                if name not in qdrant_collection_names:
                    delete_code = services.corpus.delete_by_name(db_session=db_session, name=corpus.name)
                    _print_status(f"qdrant missing '{name}', corpus deleted - result {delete_code}")
                else:
                    _print_ok("ok")


def _print_error(s: str):
    print("\x1b[1;31m" + s + "\x1b[0m")


def _print_ok(s: str):
    print("\x1b[1;32m" + s + "\x1b[0m")


def _print_status(s: str):
    print("\x1b[1;33m" + s + "\x1b[0m")


def _print_nodes(nodes):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + n.text for i, n in enumerate(nodes)]
        )
    )

cli.add_command(delete)
cli.add_command(image_caption)
cli.add_command(image_caption_dir)
cli.add_command(ingest)
cli.add_command(ingest_bm)
cli.add_command(list)
cli.add_command(list_vector)
cli.add_command(query)
cli.add_command(scan)
cli.add_command(sync)

if __name__ == "__main__":
    cli()
