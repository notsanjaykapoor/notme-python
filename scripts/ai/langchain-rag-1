#!/usr/bin/env python

import os
import sys

sys.path.insert(1, os.path.join(sys.path[0], "../.."))

import click

import langchain.agents.agent_toolkits
import langchain.callbacks.streaming_stdout
import langchain.chains
import langchain.retrievers
import langchain.retrievers.document_compressors
import langchain_community.embeddings
import langchain_community.llms
import langchain_community.vectorstores
import langchain_core.prompts
import langchain_openai.chat_models
import langchain_postgres.vectorstores
from langchain import hub

import dot_init
import services.corpus

config = {
    "notme": {
        "dir": "./data/notme/txt",
    },
    "paulg": {
        "urls": [
            "https://paulgraham.com/reddits.html",
            "https://paulgraham.com/google.html",
            "https://paulgraham.com/best.html",
        ],
    },
    "sotu": {
        "dir": "./data/sotu",
    },
}

@click.group()
def cli():
    pass

@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.option('--query', default=None, required=True, help="question")
@click.pass_context
def chat(ctx, name: str, query: str) -> dict:
    """
    """
    search_result = services.corpus.search(query="")

    if name not in search_result.names:
        _print_error(f"corpus '{name}' invalid")
        exit(1)

    db_name = f"faiss/{name}"
    embedding = langchain_community.embeddings.GPT4AllEmbeddings()

    db = langchain_community.vectorstores.FAISS.load_local(db_name, embedding, allow_dangerous_deserialization=True)

    prompt = hub.pull("hwchase17/openai-tools-agent")
    llm = langchain_openai.chat_models.ChatOpenAI(temperature = 0)

    agent_result = services.corpus.chat_agent(db=db, llm=llm, prompt=prompt)
    agent_executor = agent_result.agent_executor

    _print_status(f"corpus '{name}' query '{query}'")

    result = agent_executor.invoke({"input": query})

    print(result) #

    _print_ok(result.get("output"))


@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.option('--query', default=None, required=True, help="question")
@click.pass_context
def oneshot(ctx, name: str, query: str) -> dict:
    """
    """
    search_result = services.corpus.search(query="")

    if name not in search_result.names:
        _print_error(f"corpus '{name}' invalid")
        exit(1)

    model_path = os.environ["RAG_MODEL_PATH"]
    db_name = f"faiss/{name}"

    if "/" in model_path:
        model_name = model_path.split("/")[-1]
    else:
        model_name = model_path

    embedding = langchain_community.embeddings.GPT4AllEmbeddings(model=model_path)
    db = langchain_community.vectorstores.FAISS.load_local(db_name, embedding, allow_dangerous_deserialization=True)

    # callbacks = [langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler()]
    # llm = langchain_community.llms.Embed4All(model_name=model_path, allow_download=False)
    llm = langchain_community.llms.GPT4All(model=model_path, allow_download=False)

    # template = """Question: {question}\nAnswer: Let's think step by step."""
    # prompt = langchain_core.prompts.PromptTemplate.from_template(template)

    _print_status(f"corpus '{name}' model '{model_name}' query '{query}'")

    conversation = langchain.chains.ConversationalRetrievalChain.from_llm(
        llm,
        retriever=db.as_retriever(),
        return_source_documents=True,
        verbose=False,
    )
    result = conversation({"question": query, "chat_history": []})

    # llm_chain = langchain.chains.LLMChain(prompt=prompt, llm=llm)
    # result = llm_chain.run(query)

    print(result) #

    _print_ok(result.get("answer").strip())


@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.pass_context
def corpus_delete(ctx, name: str) -> list[str]:
    """
    
    """
    db_url = os.environ.get("DATABASE_VECTOR_URL")


    delete_code = services.corpus.delete(db_url=db_url, db_name=name)

    if delete_code == 0:
        _print_status(f"corpus delete ok")
    else:
        _print_error(f"corpus delete error {delete_code}")


@click.command()
@click.pass_context
def corpus_list(ctx) -> list[str]:
    """
    
    """
    db_url = os.environ.get("DATABASE_VECTOR_URL")

    list_result = services.corpus.list_all(db_url=db_url)

    if list_result.code == 0:
        _print_status(f"corpus list {list_result.names}")
    else:
        _print_error(f"corpus list error {list_result.code}")


@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.option('--model', default=None, required=True, help="model name")
@click.option('--type', default=None, required=True, help="retriever type, e.g. basic, compress")
@click.option('--query', default=None, required=True, help="question")
@click.pass_context
def corpus_retrieve(ctx, name: str, model: str, type: str, query: str) -> dict:
    """
    
    """
    docs_config = config.get(name, {})

    if not docs_config:
        _print_error(f"corpus {name} invalid")
        exit(1)

    if model == "gpt4all":
        embeddings = langchain_community.embeddings.GPT4AllEmbeddings()
    elif model == "openai":
        embeddings = langchain_openai.OpenAIEmbeddings()
    else:
        embeddings = langchain_community.embeddings.HuggingFaceHubEmbeddings(model=model)

    db_url = os.environ.get("DATABASE_VECTOR_URL")

    encode_result = services.corpus.name_encode(corpus=name, model=model)
    db_name = encode_result.database
 
    _print_status(f"corpus retrieve '{name}' model '{model}' db '{db_name}' query '{query}'")

    # db = langchain_postgres.vectorstores.PGVector.load_local(db_name, embeddings, allow_dangerous_deserialization=True)
    db = langchain_postgres.vectorstores.PGVector(
        embeddings=embeddings,
        collection_name=db_name,
        connection=f"{db_url}/{db_name}",
        use_jsonb=True,
    )
    db_retriever = db.as_retriever()

    if type == "compress":
        embeddings_filter = langchain.retrievers.document_compressors.EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
        compression_retriever = langchain.retrievers.ContextualCompressionRetriever(
            base_compressor=embeddings_filter, base_retriever=db_retriever
        )

        docs = compression_retriever.invoke(query)
    else:
        docs = db_retriever.invoke(query)

    _print_docs(docs)

@click.command()
@click.option('--name', default=None, required=True, help="corpus name")
@click.option('--model', default=None, required=True, help="embedding model name")
@click.pass_context
def corpus_write(ctx, name: str, model: str) -> dict:
    """
    
    """
    docs_config = config.get(name, {})

    if not docs_config:
        _print_error(f"corpus {name} invalid")
        exit(1)

    if model == "gpt4all":
        embeddings = langchain_community.embeddings.GPT4AllEmbeddings()
    elif model == "openai":
        embeddings = langchain_openai.OpenAIEmbeddings()
    else:
        embeddings = langchain_community.embeddings.HuggingFaceHubEmbeddings(model=model)

    db_url = os.environ.get("DATABASE_VECTOR_URL")

    encode_result = services.corpus.name_encode(corpus=name, model=model)
    db_name = encode_result.database

    _print_status(f"corpus write '{name}' model '{model}' database '{db_name}' config '{docs_config}'")

    write_result = services.corpus.write(db_url=db_url, db_name=db_name, embeddings=embeddings, config=docs_config)

    _print_status(f"corpus write '{name}' model '{model}' database '{db_name}' result docs {write_result.docs_count} chunks {write_result.chunks_count}")

    exit(0)

    # loader = langchain_community.document_loaders.DirectoryLoader(docs_dir)
    # loader = langchain_community.document_loaders.TextLoader(docs_list)

    # We load the document using LangChainâ€™s handy extractors, formatters, loaders, embeddings, and LLMs

    # documents = loader.load()
    # text_splitter = langchain.text_splitter.CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    # texts = text_splitter.split_documents(documents)

    # We use an OpenAI default embedding model
    # Note the code in this example does not preserve privacy
    # embeddings = OpenAIEmbeddings()

    # LangChain provides API functions to interact with FAISS
    # db = langchain_community.vectorstores.FAISS.from_documents(texts, embedding=embedding)
    # db.save_local(db_name)

    # db = langchain_community.vectorstores.FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)

    # _print_status(f"corpus {name} write to db {db_name} docs {len(documents)} chunks {db.index.ntotal}")

    # We create a 'retriever' that knows how to interact with our vector database using an augmented context
    # We could construct the retriever ourselves from first principles but it's tedious
    # Instead we'll use LangChain to create a retriever for our vector database

    # retriever = db.as_retriever()

    # tool = langchain.agents.agent_toolkits.create_retriever_tool(
    #     retriever,
    #     name,
    #    f"search and returns documents regarding the {name}"
    # )
    # tools = [tool]

    # We wrap an LLM (here OpenAI) with a conversational interface that can process augmented requests

    # llm = langchain_openai.chat_models.ChatOpenAI(temperature = 0)
    # agent_executor = langchain.agents.agent_toolkits.create_conversational_retrieval_agent(
    #     llm,
    #     tools,
    #     max_token_limit=2000, # default value
    #     verbose=True,
    # )

    # prompt = hub.pull("hwchase17/openai-tools-agent")
    # agent = langchain.agents.create_openai_tools_agent(llm, tools, prompt)
    # agent_executor = langchain.agents.AgentExecutor(agent=agent, tools=tools)

    # input = "what is NATO?"
    # result = agent_executor.invoke({"input": input})

    # Response from the model

    input = "Where did Sanjay go to college?"
    result = agent_executor.invoke({"input": input})

    print(result) #

    input = "When was NATO created?"
    result = agent_executor.invoke({"input": input})

    print(result) #

    input = "What did the president say about Nancy Pelosi?"
    result = agent_executor.invoke({"input": input})

    print(result) #


def _print_error(s: str):
    print("\x1b[1;31m" + s + "\x1b[0m")


def _print_ok(s: str):
    print("\x1b[1;32m" + s + "\x1b[0m")


def _print_status(s: str):
    print("\x1b[1;33m" + s + "\x1b[0m")


def _print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )

cli.add_command(chat)
cli.add_command(oneshot)
cli.add_command(corpus_delete)
cli.add_command(corpus_list)
cli.add_command(corpus_retrieve)
cli.add_command(corpus_write)

if __name__ == "__main__":
    cli()
